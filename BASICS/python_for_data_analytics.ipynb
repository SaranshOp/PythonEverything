{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### python for data analysis video\n",
    "\n",
    "will only be including some code / notes which i did not knew before - got a eureka out of it.    \n",
    "\n",
    "https://www.youtube.com/playlist?list=PLiC1doDIe9rCYWmH9wIEYEXXaJ4KAi3jc \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List methods \n",
    ".count()\n",
    ".sort()\n",
    ".reverse()      \n",
    "these alter the list itself     \n",
    "use reverse( list ) so that it return a new list if you need it. \n",
    "\n",
    "indexing list[4:2:-1] -> starts at 4th ele and goes till 2 but in reverse.      \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['goodnight', 'goodbye']\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "list1 = [ 'hi' , 'hello' , 'bye' , 'goodbye' , 'goodnight' , 'goodmorning' ]\n",
    "list2 = list1[4:2:-1]\n",
    "print(list2)\n",
    "list2 = list1[2:4:-1]\n",
    "print(list2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "del(list[2]),  .pop() - last item   \n",
    "\n",
    "list1 = list2.copy() -> can fool you "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['list', [1, 2, 3], 'list']\n",
      "['list', [1, 2, 3], 'list']\n",
      "['list', [1, 2, 3], 'list']\n",
      "['list', [1, 2, 3, 4], 'list']\n",
      "['list', [1, 2, 3, 4], 'list']\n",
      "['list', [1, 2, 3], 'list']\n"
     ]
    }
   ],
   "source": [
    "import copy  \n",
    "list1 = [1,2,3]\n",
    "list2 = ['list' , list1 , 'list']\n",
    "list3 = list2.copy()         # shallow copy\n",
    "# list3 = list2[:]           # same\n",
    "list4 = copy.deepcopy(list2) # import copy module\n",
    "print(list2)\n",
    "print(list3)\n",
    "print(list4)\n",
    "list1.append(4)\n",
    "print(list2)\n",
    "print(list3)\n",
    "print(list4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tuple\n",
    "\n",
    "immutable sequence of values.       \n",
    "means you can't change the values in it.        \n",
    "but you can change the values in the list inside the tuple.\n",
    "\n",
    "### string\n",
    "indivual sequence of immutable objects. <BR>\n",
    "so in methods like .lower() or .upper() it returns a new string, not changing the original one.     eg below.    \n",
    "```python  \n",
    "my_string = \"Hello world\"\n",
    "print(my_string.upper())\n",
    "my_string\n",
    "```\n",
    "\n",
    ".find() - returns the index of the first occurence of the substring.    \n",
    ".replace() - replaces the substring with another substring.\n",
    ".split(\"-\") - splits the string at the delimiter and returns a list.    \n",
    "multi line string - \"\"\" \"\"\" or ''' '''      \n",
    ".strip(\"xX\") - removes the leading and trailing xX from the string.\n",
    "\"\".join(\"string1\",\"string2\") - joins the list of strings with the string.    \n",
    "formatted string = f\"{}\" - f string.    \n",
    "```python\n",
    "name,age = \"Alice\",10\n",
    "print(f\"Hello, {name}. You are {age} years old.\")\n",
    "```\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello python\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hello world'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_string = \"Hello world\"\n",
    "print(my_string.replace('world' , 'python'))\n",
    "my_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dictionary\n",
    "key value pairs.    \n",
    "dict.items() - returns a list of tuples. both key and value pairs.      \n",
    "\n",
    "real world data is usually in the form of dictionaries. with column names as keys and values as the data.\n",
    "'''python\n",
    "data = {\n",
    "    \"name\" : [\"Alice\",\"Bob\",\"Charlie\"],\n",
    "    \"age\" : [10,20,30]\n",
    "}\n",
    "```'    \n",
    "certain data structures like xml, json are converted to dictionaries. convienient to work with.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set\n",
    "\n",
    "sets are ordered, mutable, no duplicates collection of immutable objects.       \n",
    "set1 = {1,2,3,4}        \n",
    "set2 = {3,4,5,6}        \n",
    "set1.union(set2) - returns a new set with all the elements of both the sets.    \n",
    "do not support indexing.        \n",
    "but has .add() and .remove() methods. \"in\" , \"not in\" operators can be used.\n",
    "\n",
    ".union() , .intersection()\n",
    ".difference() - elements in set1 but not in set2        \n",
    ".symmetric_difference() - elements in set1 or set2 but not in both. <br>\n",
    ".issubset() , .issuperset() , .isdisjoint() - for checking the relationship between the sets.        <br>\n",
    "set1.update(set2) - updates set1 with the elements of set2. <br>\n",
    "set1.intersection_update(set2) - updates set1 with the intersection of set1 and set2. <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### numpy\n",
    "\n",
    "arr1 = np.array(list1)      \n",
    ".shape , .size - shape of the array and size of the array. <br>\n",
    ".dtype\n",
    "np.zeros((2,3)) - creates a 2x3 array of zeros. <br>\n",
    "np.identity(3) - creates a 3x3 identity matrix. <br>\n",
    "indexing and slicing is possible. <br>\n",
    "\n",
    "```python\n",
    "one_d_array = np.array([1,2,3,4,5])\n",
    "two_d_array = np.array([one_d_array, one_d_array+6])\n",
    "two_d_array[1,2] # 9\n",
    "two_d_array[1,1:3] # [8,9]\n",
    "two_d_array[::-1,::-1] # reverse the array  \n",
    "```\n",
    ".reshape() - reshapes the array. <br>\n",
    ".flatten() - flattens the array. <br> \n",
    "np.ravel(a=,order=) - same as flatten but can also do it by column (order=F) <br>\n",
    "\n",
    ".T = transpose of the array. <br>\n",
    ".flipud() , .fliplr() - flips the array up down and left right. <br>\n",
    ".rot90(k=) - rotates the array by 90 degrees k times. <br>\n",
    "np.roll(a=,shift=) - rolls the array by shift times. <br>\n",
    "arr1.concatenate(arr2,axis=,out=) - concatenates the arrays, axis=0 for vertical, axis=1 for horizontal <br>\n",
    "```python\n",
    "arr1 = np.array([[1,2],[3,4]])\n",
    "arr2 = np.array([[5,6],[7,8]])\n",
    "np.concatenate((arr1,arr2),axis=0)\n",
    "np.concatenate((arr1,arr2),axis=1)\n",
    "arr1.concatenate(arr2,axis=0)\n",
    "```\n",
    "\n",
    "math operations are elemeent wise and faster, memory efficient in numpy <br>\n",
    "\n",
    "two_d_array.sum(axis=0) - sum of the columns <br>\n",
    "two_d_array.sum(axis=1) - sum of the rows <br>\n",
    "two_d_array.mean(axis=0)\n",
    ".std() , .var() - standard deviation and variance <br>\n",
    ".min() , .max() - min and max <br>\n",
    ".argmin() , .argmax() - index of min and max <br>\n",
    ".argsort() - returns the indices of the sorted array <br>\n",
    "np.where(condition) - returns the indices of the elements which satisfy the condition <br>\n",
    "\n",
    "np.random.rand(2,3) - random numbers between 0 and 1 , returns a 2x3 array <br>\n",
    "np.random.randn(2,3) - random numbers from normal distribution <br>\n",
    "np.random.randint(1,10,(2,3)) - random integers between 1 and 10 , returns a 2x3 array <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 4]\n",
      " [5 6]\n",
      " [7 8]]\n",
      "[[1 2 5 6]\n",
      " [3 4 7 8]]\n",
      "[[1 2]\n",
      " [3 4]]\n"
     ]
    }
   ],
   "source": [
    "#%pip install numpy\n",
    "import numpy as np\n",
    "arr1 = np.array([[1,2],[3,4]])\n",
    "arr2 = np.array([[5,6],[7,8]])\n",
    "print(np.concatenate((arr1,arr2),axis=0)) # axis=0 mean concatenate column wise\n",
    "print(np.concatenate((arr1,arr2),axis=1)) # axis=1 mean concatenate row wise\n",
    "# arr1.concatenate(arr2,axis=0) # error \n",
    "print(arr1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pandas\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pandas\n",
    "#### pandas series\n",
    "one dimensional labeled array. <br>\n",
    "s = pd.Series([1,2,3,4],index=[\"a\",\"b\",\"c\",\"d\"]) <br>\n",
    "dict= {\"a\":1,\"b\":2,\"c\":3,\"d\":4} <br>\n",
    "s = pd.Series(dict) <br>\n",
    "\n",
    "```python\n",
    "my_dict = {\n",
    "    \"name\" : [\"Alice\",\"Bob\",\"Charlie\"],\n",
    "    \"age\" : np.array([10,20,30]),\n",
    "    'weight' : pd.Series([20,30,40],index=[\"A\",\"B\",\"C\"])\n",
    "    'height' : pd.Series([20,30,40],index=[\"al\",\"bo\",\"ch\"]) # error, index shpuld be same and complusory\n",
    "    'sibilings' : 1,\n",
    "    'gender' : \"F\"\n",
    "}\n",
    "df = pd.DataFrame(my_dict)\n",
    "```\n",
    "df[\"name\"] , df.name - both are same. <br>\n",
    "del df[\"name\"] - deletes the column. <br>\n",
    "\n",
    "df.loc[\"A\"] , df.iloc[0] - both in our case is same. but loc is label based and iloc is index based. so if row shuffled, iloc will give the 1st row, but loc will give the row with label A. <br>\n",
    "\n",
    "df.loc[\"A\":\"B\",\"name\":\"age\"] - slicing the dataframe. <br>\n",
    "df.iloc[0:2,0:2] - slicing the dataframe. <br>\n",
    "\n",
    "logical indexing -\n",
    "bool_index = df[\"age\"] > 20 <br>\n",
    "df[bool_index]  <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a    1\n",
      "b    2\n",
      "c    3\n",
      "d    4\n",
      "dtype: int64\n",
      "      name  age  weight  height  no  sibilings\n",
      "A    Alice   10      20       1   1          1\n",
      "B      Bob   20      30       2   2          1\n",
      "C  Charlie   30      40       3   3          1\n"
     ]
    }
   ],
   "source": [
    "dict= {\"a\":1,\"b\":2,\"c\":3,\"d\":4} \n",
    "s = pd.Series(dict) \n",
    "print(s)\n",
    "my_dict = {\n",
    "    \"name\" : [\"Alice\",\"Bob\",\"Charlie\"],\n",
    "    \"age\" : np.array([10,20,30]),\n",
    "    'weight' : pd.Series([20,30,40],index=[\"A\",\"B\",\"C\"]),\n",
    "    'height' : [1,2,3],\n",
    "    'no' : pd.Series([1,2,3],index=[\"A\",\"B\",\"C\"]), # index should be same and complusory\n",
    "    'sibilings' : 1,\n",
    "}\n",
    "df = pd.DataFrame(my_dict)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import os <br>\n",
    "os.getcwd() - get the current working directory <br>\n",
    "os.chdir(/kaggle/) <br>\n",
    "os.listdir(/kaggle/input/) - list the files in the directory <br>\n",
    "\n",
    "pd.read_table(\"file.txt\",sep=\"\") - \n",
    "pd.read_csv(\"file.csv\") -   \n",
    "pd.read_excel(\"file.xlsx\", sheet=\"sheet1\")      \n",
    "pd.read_sql(\"select * from table\",connection) - reads the sql query from the database. <br>\n",
    "pd.read_clipboard() - reads the data from the clipboard. <br>\n",
    "pd.read_html(\"url\") - reads the html table from the url. needs beautifulsoup4 and html5lib. <br>\n",
    "draft.to_csv(\"file.csv\") - writes the dataframe to the csv file. <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make -ve numbers zero of some sort of transformation - df[df<0] = 0 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : -0.33621157217989395\n",
      "4 : -0.8030256050833602\n",
      "5 : -0.9782935965408843\n",
      "6 : -0.1883420264137099\n",
      "7 : -0.8758897202186617\n",
      "8 : -0.2655239816925956\n",
      "[[0.         0.63787072 0.21416869]\n",
      " [0.97081354 0.         0.        ]\n",
      " [0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "my_data = np.random.uniform(-1,1,(3,3)) # 5x5 matrix with random values between -1 and 1\n",
    "for i,data in enumerate(my_data.flatten()): # flatten() convert 2d array to 1d array, enumerate() return index and value\n",
    "    if data < 0:\n",
    "        print(i,data, sep=\" : \")\n",
    "        my_data.flat[i] = 0 # flat is a 1d array of my_data, so we can change value, flat is a view of my_data\n",
    "print(my_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.90972128, 0.76577167],\n",
       "       [0.        , 0.48328731, 0.80300994],\n",
       "       [0.09672036, 0.        , 0.        ]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_data = np.random.uniform(-1,1,(3,3))\n",
    "np.where(my_data < 0, 0, my_data) # if data is less than 0 then replace with 0 else keep same\n",
    "# substantailly faster than above loopinng method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def sum(*args):\n",
    "    print(type(args))\n",
    "    return sum(args)\n",
    "sum(1,2,3,4)\n",
    "\n",
    "def keywordargs(**kwargs): # keyword arguments - \n",
    "    print(type(kwargs))\n",
    "    print(kwargs.keys())\n",
    "    print(kwargs.values())\n",
    "    return sum(kwargs.values())\n",
    "keywordargs(a=1,b=2,c=3) \n",
    "\n",
    "(lambda x, y : x + y) # can be used along with map, filter, reduce\n",
    "\n",
    "map(lambda x : x**2, [1,2,3,4])\n",
    "filter(lambda x : x%2==0, [1,2,3,4])\n",
    "reduce(lambda x,y : x+y, [1,2,3,4])\n",
    "df[\"age\"].apply(lambda x : x**2) # apply the lambda function to the series\n",
    "\n",
    "# eg with increase complexity\n",
    "\n",
    "map(lambda x : x**2, df[\"age\"]) # apply the lambda function to the series returns a map object\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(x, y)>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### list comprehension\n",
    "[expression for item in iterable if condition] <br>\n",
    "```python\n",
    "squares = [x**2 for x in range(10)]\n",
    "squares = [x**2 for x in range(10) if x%2==0]\n",
    "comboned =[x+y for x in \"life\" for y in \"sucks\"]\n",
    "```\n",
    "#### dictionary comprehension\n",
    "{key:value for item in iterable if condition} <br>\n",
    "```python\n",
    "squares = {x:x**2 for x in range(10) if x%2==0}\n",
    "comboned = {x:y for x in \"life\" for y in \"sucks\"}\n",
    "```\n",
    "zip() - combines the two lists into a list of tuples. <br>\n",
    "```python\n",
    "zipped = zip(list1,list2) # returns tuple made elements wise\n",
    "my_dict = {key:value for key,value in zipped}\n",
    "unzipped = zip(*zipped)\n",
    "list(unzipped)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'life': 'suck', ' ': 'suck', ';': 'suck'}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comboned = {x:y for x in [\"life\",\" \",\";\"] for y in [\"suck\"]}\n",
    "comboned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration and Cleaning\n",
    "[Kaggle Notebook](https://www.kaggle.com/code/saranshkathal/python-for-data-14-data-exploration-and-cleaning/edit) <br>\n",
    "\n",
    "Exploring the variables:\n",
    "    use .shape(), .dtypes(), .describe(), .info() \n",
    "Qs : \n",
    "    Do I Need All of the Variables? \n",
    "        Can i drop some columns ? variables ?\n",
    "        Look at all variable closely.\n",
    "        Is there a Pattern?\n",
    "        \n",
    "    Should I Transform Any Variables?\n",
    "        pd.Categorical(), pd.to_datetime()\n",
    "        use .unique(), .describe() after transformation\n",
    "        use comprehensions, lambda functions, map, filter, reduce for transformation\n",
    "\n",
    "    Are there NA Values, Outliers or Other Strange Values?\n",
    "        .isna(), .isnull(), .fillna(), .dropna()\n",
    "        .drop_duplicates()\n",
    "        .value_counts()\n",
    "        .plot(kind=\"bar\"), .plot(kind=\"hist\"), variable.hist(col=,figsize=,bins=) , variable.boxplot(), variable.plot(kind=\"scatter\",x=,y=) \n",
    "        can fill mean , meadian after checking for normal distribution\n",
    "        can use to find index and fill values using np.where(condition,fill_value_true,fill_value_false)\n",
    "\n",
    "    Should I Create New Variables?\n",
    "        Creating a new variable can be as simple as taking one variable and adding, multiplying or dividing by another\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " .describe() on Categoical data <br>\n",
    "```python\n",
    "categorical = titanic_train.dtypes[titanic_train.dtypes == \"object\"].index\n",
    "print(categorical)\n",
    "\n",
    "titanic_train[categorical].describe()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filling in the missing values - \n",
    "there is often a no right way to fill in the missing values it a matter of judgement and having experience in your domain. <br>\n",
    "Similar to NA values, there's no single cure for outliers. You can keep them, delete them or transform them in some way to try to reduce their impact. Even if you decide to keep outliers unchanged it is still worth identifying them since they can have disproportionately large influence on your results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "new_survive = pd.Categorical(df[\"survived\"]) #converts the column to categorical.\n",
    "new_survive = df[\"survived\"].rename_categories([\"Died\",\"Survived\"])  # renames the categories. <br>\n",
    "new_survive.describe() \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Working with Text Data\n",
    "pandas has a lot of built-in methods for working with text data. <br>\n",
    "pd.Series() - can be used to convert the list to series. <br>\n",
    "then        \n",
    "> comments.str.lower() or comments[0].lower() <br> \n",
    "similarily .lower(), .upper(), .str.len(), .strip(\"[ ]\"), .str.cat(), .str.split(), \n",
    "can make a logica expressions / indexing  using .contains(\"virat|kohli\"), .startswith(\"virat\"), .endswith(\"kohli\") <br> \n",
    "\n",
    ".contains(\".ill\") - returns the rows which contains the string \"ill\" <br>\n",
    "[Tt] - returns the rows which contains the string \"T\" or \"t\" <br>\n",
    "etc. \n",
    "\n",
    "using re or use any other from online.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### working with numerical data\n",
    "Very Good for knowing some basic steps and why - scaling / substracting mean, skewed data transformations - log and sqrt , correlation variables and what to do with them and lastly imputation techniques. and code for all. <br>\n",
    "[Youtube](https://www.youtube.com/watch?v=Y1mXf0rXm04&list=PLiC1doDIe9rCYWmH9wIEYEXXaJ4KAi3jc&index=16)\n",
    "[| Much better and faster read this kaggle nb](https://www.kaggle.com/code/hamelg/python-for-data-16-preparing-numeric-data)\n",
    "\n",
    "\n",
    "tranformations on numeric data - \n",
    "```python\n",
    "(data+1).apply(np.log) # log transformation or use np.log1p(data) , 1 + in  log1p is to avoid log(0) error amnd negative values\n",
    "data.apply(np.sqrt) # sqrt transformation\n",
    "data.apply(lambda x : x**0.33) # cube root transformation\n",
    "```\n",
    "##### correlation\n",
    "correlation is a measure of how two variables change together. and typically due to some underlying relationship. (but causation v/s correlation) <br>\n",
    "\n",
    ".corr() , scatter_matrix() , sns.pairplot() <br>\n",
    "scatter plot is a good way to look at data and sopt interesting relationships. <br>\n",
    "\n",
    "high correlation can be a problem in some models and can interfere and skrew up results. <br>\n",
    "\n",
    "leave them be, drop one of the variables, combine the variables, transform the variables. <br>\n",
    "dimentionality reduction (PCA) - can be used to reduce the number of variables. <br>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputation\n",
    "use np.where(condition,fill_value_true,fill_value_false) <br>\n",
    "Imputer() - from sklearn.preprocessing  - strategy = \"mean\" , \"median\" , \"most_frequent\" <br>\n",
    "k nearest neighbours - from sklearn.impute - KNNImputer() - but it significantly increases the complexity time as it itself is a predictive model, and may not increase the yeaild significantly. <br>\n",
    "IterativeImputer() - from sklearn.impute - uses a predictive model to fill in the missing values. <br>\n",
    "\n",
    "#### Scaling\n",
    "scaling is important because many machine learning algorithms don't perform well when the input numerical variables have different scales. <br>\n",
    "StandardScaler() - from sklearn.preprocessing - scales the data so that it has a mean of 0 and a standard deviation of 1. <br>\n",
    "MinMaxScaler() - scales the data so that all the values are between 0 and 1. <br>\n",
    "RobustScaler() - scales the data so that it is robust to outliers. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dealing with dates\n",
    "df = pd.read_csv(\"file.csv\",parse_dates=[\"date\", 0]) - state the column name or number which has the date. <br>\n",
    "\n",
    "od pd.to_datetime(data,format=\"%H:%M:%S %Y-%m-%d\") - the format is the one in which the date data is present. <br>\n",
    "https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html\n",
    "\n",
    "\n",
    "we can extract the year, month, day, weekday, etc. <br> \n",
    "df[\"date\"].dt.year , df[\"date\"].dt.month , df[\"date\"].dt.day , df[\"date\"].dt.weekday , df[\"date\"].dt.quarter <br>\n",
    "\n",
    "print(dates.iloc[0] - dates.iloc[1]) - gives the difference in the dates. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### merge and join\n",
    "pd.merge(df1,df2,on=[\"column_name\", \"col_2\"],how=\"\") - merges the two dataframes on the column name. <br>\n",
    "how = \"inner\" -> intersection, \"outer\" -> union, \"left\" -> left join, \"right\" -> right join <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
